\documentclass[expanded]{lkx_pset}

\title{CS181 Problem Set 1}
\author{Lev Kruglyak}
\due{February 10, 2024}

\input{../cs181.sty}

\begin{document}
\maketitle

\begin{problem}{1}[Setting up the Regression]
  Your goal is to predict temperature variation given the year.  Before we start deriving and coding up our regressions, we will interrogate the set-up of our problem.  
\end{problem}

\begin{parts}
  \begin{part}{a}
    These data were derived from ice core samples in Antarctica.
    Take a brief look at the
    \href{https://www.ncei.noaa.gov/pub/data/paleo/icecore/antarctica/epica_domec/edc3deuttemp2007.txt}{original
      data file}. Briefly discuss how the data were processed: what kinds of
    decisions and corrections had to be made?  We know that different
    places on earth have different temperatures: what does the
    temperature in the temperature column correspond to?
  \end{part}
        
  \begin{part}{b}
    Even before doing any formal regressions, we see that there is
    some periodicity in the data: there are years that are warmer, and
    years that are cooler.  Suppose you are a government official
    advising on how much to worry about climate change.  Would it be
    reasonable to use this pattern as evidence that the earth will
    cool down again?  Why or why not, or to what extent?
  \end{part}


  \begin{part}{c}
    In the problems below, we will focus on interpolating
    temperatures for years not provided in the training set.  What
    kind of application would such a regression be useful for?
  \end{part}
\end{parts}

\begin{problem}{2}[Optimizing a Kernel]
Kernel-based regression techniques are similar to nearest-neighbor
regressors: rather than fit a parametric model, they predict values
for new data points by interpolating values from existing points in
the training set.  In this problem, we will consider a kernel-based
regressor of the form:
\begin{equation*}
  f_\tau(x^*) = \cfrac{\sum_{n} K_\tau(x_n,x^*) y_n}{\sum_n K_\tau(x_n, x^*)} 
\end{equation*}
where $\mathcal{D}_\texttt{train} = \{(x_n,y_n)\}_{n = 1} ^N$ are the
training data points, and $x^*$ is the point for which you want to
make the prediction.  The kernel $K_\tau(x,x')$ is a function that
defines the similarity between two inputs $x$ and $x'$. A popular
choice of kernel is a function that decays as the distance between the
two points increases, such as
\begin{equation*}
  K_\tau(x,x') = \exp\left\{-\frac{(x-x')^2}{\tau}\right\}
\end{equation*}
where $\tau$ represents the square of the lengthscale (a scalar value that 
dictates how quickly the kernel decays).  In this
problem, we will consider optimizing what that (squared) lengthscale
should be.
\end{problem}
 
\begin{parts}
  \begin{part}{a}
    Let's first take a look at the behavior of the fitted model for different values of $\tau$. Implement the \texttt{kernel\_regressor} function in the notebook, and plot your model for years in the range $800,000$ BC to $400,000$ BC at $1000$ year intervals for the following three values of $\tau$: $1, 50, 2500$. 
    Since we're working in terms of thousands of years, this means you should plot $(x, f_\tau(x))$ for $x = 400, 401, \dots, 800$. \textbf{The plotting has been set up} for you in the notebook already. \textbf{In no more than 5 sentences}, describe how the fits change with $\tau$.
  \end{part}

  \begin{part}{b} 
    Now, we will evaluate the quality of each model \emph{quantitatively} by computing the error on some test set $\mathcal{D}_\texttt{test} = \{(x'_m, y'_m)\}_{m = 1} ^M$.  Write down the expression for MSE of $f_\tau$ over the test set as a function of the training set and test set. Your answer may include $\{(x'_m, y'_m)\}_{m = 1} ^M$, $\{(x_n, y_n)\}_{n = 1} ^N$, and $K_\tau$, but not $f_\tau$.
  \end{part}

  \begin{part}{c}
    Suppose we used the training set as our test set, that is, we evaluated the expression above with $\mathcal{D}_\texttt{test} = \mathcal{D}_\texttt{train}$, and chose the value of $\tau$ which gave the smallest loss.  What value of $\tau$ would be picked?  Why is setting $\mathcal{D}_\texttt{test} = \mathcal{D}_\texttt{train}$ a bad idea?
  \end{part}

  % \emph{Hint: consider what value of $\tau$ would be optimal, for $\tau$ ranging in $(0, \infty)$. We can consider $f_\tau(x^*)$ as a weighted average of the training responses, where the weights are proportional to the distance to $x^*$, and the distance is computed via the kernel. What happens to $K_\tau(x, x')$ as $\tau$ becomes very small, when $x = x'$? What about when $x \neq x'$?}

  \begin{part}{d}
    Let us compute the MSE on the provided test set (that is, not the training set). Write Python code to compute the MSE with respect to the same lengthscales as in Part 1. Which model yields the lowest test set MSE? 
  \end{part}

  \begin{part}{e}
    Describe the time and space complexity of kernelized regression with respect to the size of the training set $N$. 
    How, if at all, does the size of the model---everything that needs to be stored to make predictions---change with the size of the training set $N$? 
    How, if at all, do the number of computations required to make a prediction for some input $x^*$ change with the size of the training set $N$?
  \end{part}
\end{parts}

\begin{problem}{3}[Kernels and kNN]
  Now, let us compare the kernel-based approach to an approach based on
  nearest-neighbors.  Recall that kNN uses a predictor of the form
  \begin{equation*}
    f(x^*) = \frac{1}{k} \sum_n y_n \mathbb{I}(x_n \texttt{ is one of k-closest to } x^*)
  \end{equation*}

  \noindent where $\mathbb{I}$ is an indicator variable. For this problem, you will use the \textbf{same dataset as in Problem 1}. Write your own tests!
\end{problem}

\begin{parts}
  \begin{part}{a}
    The kNN implementation \textbf{has been provided for you} in the notebook. Run the cells to plot the results for $k=\{1, 3, N-1\}$, where $N$ is the size of the dataset.

    Describe what you see: what is the behavior of the functions in
    these three plots?  How does it compare to the behavior of the
    functions in the three plots from Problem 1? In particular, are
    there situations in which kNN and kernel-based regression
    interpolate similarly?
  \end{part}

  \begin{part}{b}
    Compute the MSE on the test set for each value of $k$.  Which solution has the lowest MSE?  
  \end{part}

  \begin{part}{c}
    As in Problem 1, describe the space and time complexity of kNN.  How does what is stored to compute predictions change with the size of the training set $N$?  How does the computation needed to compute the prediction for a new input depend on the size of the training set $N$? (For the latter, justify based off of your implementation.)
  \end{part}
\end{parts}

\begin{problem}{4}[Modeling Climate Change 800,000 Years Ago]
Our last regression will be linear regression.  We currently only have
a one dimensional input, the year.  To create a more expressive linear
model, we will introduce basis functions.
\end{problem}

\begin{parts}
  \begin{part}{a}
    We will first implement the four basis regressions below. (The first basis has been implemented for you in the notebook as an example.) Note that we introduce an addition transform $f$ (already into the provided notebook) to address concerns about numerical instabilities.
    \begin{enumerate}
      \item $\phi_j(x)= f(x)^j$ for $j=1,\ldots, 9$. $f(x) = \frac{x}{1.81 \cdot 10^{2}}.$
      \item $\phi_j(x) = \exp\left\{-\cfrac{(f(x)-\mu_j)^2}{5}\right\}$ for $\mu_j=\frac{j + 7}{8}$ with $j=1,\ldots, 9$. $f(x) = \frac{x}{4.00 \cdot 10^{2}}.$
      \item $\phi_j(x) =  \cos(f(x) / j)$ for $j=1, \ldots, 9$. $f(x) = \frac{x}{1.81}$.
      \item $\phi_j(x) = \cos(f(x) / j)$ for $j=1, \ldots, 49$. $f(x) = \frac{x}{1.81 \cdot 10^{-1}}$. \footnote{For the trigonometric bases (c) and (d), the periodic nature of
    cosine requires us to transform the data such that the 
    lengthscale is within the periods of each element of our basis.}
    \end{enumerate}

    {\footnotesize * Note: Please make sure to add a bias term for
    all your basis functions above in your implementation of the 
    \verb|make_basis|.}

    Let $$ \mathbf{\phi}(\mathbf{X}) = 
    \begin{bmatrix} 
    \mathbf{\phi}(x_1) \\
    \mathbf{\phi}(x_2) \\
    \vdots \\
    \mathbf{\phi}(x_N) \\
    \end{bmatrix} \in \mathbb{R}^{N\times D}.$$
    You will complete the \verb|make_basis| function which must return $\phi(\mathbf{X})$ for each part (a) - (d). You do NOT need to submit this code in your \LaTeX writeup.

    For each basis create a plot of your code graphing the least squares regression line trained on your training data against a scatter plot of the training data. Boilerplate plotting code is provided in the notebook.
  \end{part}

  \begin{part}{b}
    Now we have trained each of our basis regressions.  For each basis
    regression, compute the MSE on the test set.  Discuss: do any of the
    bases seem to overfit?  Underfit?  Why?
  \end{part}

  \begin{part}{c}
    Briefly describe what purpose the transforms $f$ serve: why are they helpful?
  \end{part}

  \begin{part}{d}
    As in Problems 1 and 2, describe the space and time complexity of linear regression.  How does what is stored to compute predictions change with the size of the training set $N$?  How does the computation needed to compute the prediction for a new input depend on the size of the training set $N$?  How do these complexities compare to those of the kNN and kernelized regressor?
  \end{part}

  \begin{part}{e}
    Briefly compare and constrast the different regressors: kNN,
    kernelized regression, and linear regression (with bases).  Are some
    regressions clearly worse than others?  Is there one best
    regression?  How would you use the fact that you have these multiple
    regression functions?
  \end{part}

% Note:
% Recall that we are using a 
% different set of inputs $\mathbf{X}$ for each basis (a)-(d). 
% Although it may seem as though this prevents us from being able 
% to directly compare the MSE since we are using different data, 
% each transformation can be considered as being a part of our model. 
% Contrast this with transformations (such as standardization) that cause the variance of the target $\mathbf{y}$ to be different; in these cases the
% MSE can no longer be directly compared.
\end{parts}

\begin{problem}{5}[Deriving Linear Regression]
  In the previous problems, you focused on implementing regressions
  and exploring their fits on data. Now we will turn to some more
  analytic work.  Specifically, the solution for the least squares
  linear regressions ``looks'' kind of like a ratio of covariance and
  variance terms.  In this problem, we will make that connection more
  explicit.

  \medskip
  Let us assume that our data are tuples of scalars $(x,y)$ that are
  described by some joint distribution $p(x,y)$. We will consider the process of fitting these data from this distribution with the best linear model
  possible, that is a linear model of the form $\hat{y} = wx$ that
  minimizes the expected squared loss $E_{x,y}[ ( y - \hat{y} )^2
  ]$.
\end{problem}
  
\begin{parts}
  \begin{part}{a}
    Derive an expression for the optimal $w$, that is, the $w$
    that minimizes the expected squared loss above.  You should leave
    your answer in terms of moments of the distribution, e.g. terms
    like $E_x[x]$, $E_x[x^2]$, $E_y[y]$, $E_y[y^2]$, $E_{x,y}[xy]$
    etc.
  \end{part}

  \begin{part}{b}
    Provide unbiased and consistent formulas to estimate $E_{x, y}[yx]$
   and $E_x[x^2]$ given observed data $\{(x_n,y_n)\}_{n=1}^N$.
  \end{part}

  \begin{part}{c} In general, moment terms like $E_{x, y}[yx]$, $E_{x, y}[x^2]$,
    $E_{x, y}[yx^3]$, $E_{x, y}[\frac{x}{y}]$, etc. can easily be
    estimated from the data (like you did above).  If you substitute in
    these empirical moments, how does your expression for the optimal
    $w^*$ in this problem compare with the optimal $w^*$ that we see in
    Section 2.6 of the cs181-textbook?
  \end{part}

  \begin{part}{d}
    Many common probabilistic linear regression models assume that
    variables x and y are jointly Gaussian.  Did any of your above
    derivations rely on the assumption that x and y are jointly
    Gaussian?  Why or why not?
  \end{part}
\end{parts}

\end{document}
