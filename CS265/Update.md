# Abstract Design

  * Learned and classical structures consist of stacked `layers of nodes`, each node has a `maximal key`. These layers can be built on top of any other `layer of nodes`, since the collection of maximal keys provides a common interface.
  
Search:

  * Approach 1 (Bootstrap, simpler, only supports search) a classical layer returns an exact pointer to a node in the layer below, whereas learned (PGM) models return a range of pointers to the nodes below. For read-only, or immutable, clearned data structures, these can both be considered as ranges, since a pointer `ptr` is the same as a range `[ptr, ptr+1)`. This allows for a common interface for BTree and PGM layers, and thus enables a full, efficient implementation of an immutable clearned structure.

  * Approach 2 (Complex, but efficient and supports insertion): Alternatively, we could define layers as mapping pointers to pointers. When a classical node returns an approximate position, we do the binary search `inside the layer` and return a pointer. While this is much more complex at a type and safety level, (implmenting this without overhead requires some very advanced Rust) this is much more flexible when inserts must also be supported, since it gives BTree layers the flexbility to split very easily. More generally, storing layers of nodes continguously presents a problem for inserts, which is the case with Approach 1. A main reason why BTree inserts are efficient is because the nodes in each layer don't have to be stored contiguously, and thus splitting a node can be simply a matter of allocating a node, and updating pointers. On the other hand, in a contiguously stored layer, we might have to move all of the following nodes aside to make room for new nodes. PGM layers on the other hand effectively require that the data below them is stored contiguously to be able to perform a binary search and decide which node in the lower layer to use. Otherwise, we might have to incur as many as O(E) disk reads PER LEVEL, which is unnacceptably bad since common Epsilon factors can be as high as 64. Thus there must be some sort of tradeoff, here are some ideas:
    1. we introduce some kind of `maximal key cache layer` which avoids the large memory overhead when searching, but adds an overhead to maintain it during insertion
    2. nodes aren't stored fully coniguously, but in pages which have a fill factor. This works perfectly for layers indexed by BTree nodes, but for data indexed by PGM layers, we need some sort of `TLB`-like structure to transform from the `node rank (index in layer)` to a (pointer, index) pair within its page.

So far, we've implemented Approach 1 and we're working on Approach 2 for the mutable version of the clearned data structure.

# General design principles (more about this in the final paper)

1. Optimal Materialization of Pure Designs: both pure learned and pure classical designs generated by the engine should be near optimal. For BTree's, a target to reach is the std::collections::BTreeMap implementation, or the more advanced `sled` crate implementation. For PGM's, this is the official C++ repository implementation. If the pure materialized designs are near optimal, the hybrid designs are as well.

2. Isolate design descisions as generics, combine with macros: Every time we encounter a design decision that isn't obvious: For instance wether or not to use a `key cache` in between layers, we try to isolate it as some generic configuration, either using ZST markers, or some const type parameters. This allows for beter safety for the modular components, and then later these components can be combined by a procedural macro. As in final report:

"Most approaches to such a project of dynamic design materialization must incorporate some level of dynamic code generation. By dynamic code generation, we mean a form of code synthesis which operates at the level of in- dividual lines of code; say an algorithm which combines chunks of code with some variable replacements. While some degree of such “unhygenic” code is required for a project of this scope, we would like this to be kept at a minimum. Considering the full scope of the design space we are materializing, it’s critical that all components be unit testable and complexity be kept confined to understandable components.
An axiom of this project is to split larger designs into highly-optimized, testable generic components which are already present in BTrees and learned indexes, and then scarcely employ dynamic code generation to combine these components. These components should also have compatible external interfaces which allow for the macros to easily synthesize them into a single design. There should also be restrictions on generic parameters, which allows us to enforce reasonable design rules at compile time.
This principle minimizes much of the inherent complex- ity in automatic design materialization, and allows us to borrow design principles directly from BTrees and learned indexes."

3. On-site Materialization: From a user perspective, we would like the macro which invokes code generation of an index design to not pollute the namespace or module where it was invoked. As long as the design parameters are known at compile time, it should be possible to create as many hybrid index designs in a project as a user desires.
This especially eliminates the use of hard-coded global variables which dictate design decisions; we want these design decisions to be either explicitly determined by the user, or for them to be inferred based on the given parame- ters. While this does substantially increase the complexity
of the code, it allows for far easier benchmarking and unit testing, which is a core feature of this project.

# Why Rust

Is this section needed or is it obvious?
